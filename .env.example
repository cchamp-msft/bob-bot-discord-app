# Discord Bot Configuration
DISCORD_TOKEN=your_bot_token_here
DISCORD_CLIENT_ID=your_client_id_here

# API Endpoints
COMFYUI_ENDPOINT=http://localhost:8190
OLLAMA_ENDPOINT=http://localhost:11434

# AccuWeather Configuration
# ACCUWEATHER_API_KEY=your_accuweather_api_key_here
# ACCUWEATHER_DEFAULT_LOCATION=Seattle
# ACCUWEATHER_ENDPOINT=http://dataservice.accuweather.com

# NFL / ESPN Configuration (free, no API key required)
# NFL_BASE_URL=https://site.api.espn.com/apis/site/v2/sports/football/nfl
# NFL_ENABLED=true

# Meme (memegen.link) Configuration (free, no API key required)
# Templates are cached locally and refreshed every 7 days.
# MEME_BASE_URL=https://api.memegen.link
# MEME_ENABLED=true
# MEME_LOGGING_DEBUG=false  # Log full meme inference prompts and parsed payload

# SerpAPI Configuration (Google Search + AI Overview)
# SERPAPI_API_KEY=your_serpapi_key_here
# SERPAPI_ENDPOINT=https://serpapi.com
# AI Overview availability is locale-dependent; defaults to en/us when unset.
# SERPAPI_HL=en
# SERPAPI_GL=us
# Optional location hint can improve AI Overview coverage for some queries.
# SERPAPI_LOCATION=United States

# HTTP Server Configuration
HTTP_PORT=3000
# Configurator is localhost-only by default
HTTP_HOST=127.0.0.1

# Configurator UI theme (dark, grayscale, green, orange, purple, black-cyan)
# CONFIGURATOR_THEME=dark

# Admin auth token — STRONGLY RECOMMENDED when the configurator is reachable
# through a reverse proxy. When set, every configurator/admin request must
# include an "Authorization: Bearer <token>" header. When unset the
# configurator relies on the localhost-only IP guard (legacy behaviour).
# Generate a secure random value, e.g.: openssl rand -hex 32
# ADMIN_TOKEN=

# Outputs file server (serves generated images for Discord)
OUTPUTS_PORT=3003
# Bind to all interfaces so Discord can fetch images
OUTPUTS_HOST=0.0.0.0
# Trust proxy setting for outputs server rate limiting:
#   false (default) = use direct socket IP only
#   true            = trust all proxies
#   1,2,...         = trust that many proxy hops
# Enable only behind a trusted reverse proxy/load balancer.
# OUTPUTS_TRUST_PROXY=false
# Set OUTPUT_BASE_URL to the externally-reachable URL for generated images.
# Behind an SSL reverse proxy this should be https://your-public-host/
OUTPUT_BASE_URL=http://localhost:3003

# File Configuration
FILE_SIZE_THRESHOLD=10485760

# Bot Configuration
DEFAULT_TIMEOUT=300

# Path to the runtime tools configuration file (XML format). Default: config/tools.xml
# On first run, config/tools.default.xml is copied to config/tools.xml
# if the runtime file does not exist (mirrors .env.example → .env pattern).
# TOOLS_CONFIG_PATH=config/tools.xml

# Max file attachments per Discord message (1–10, Discord limit is 10)
MAX_ATTACHMENTS=10

# Ollama Model (discovered via configurator Test button)
OLLAMA_MODEL=

# Error Handling
ERROR_MESSAGE=I'm experiencing technical difficulties. Please try again later.
ERROR_RATE_LIMIT_MINUTES=60

# Reply Chain Context (traverses Discord reply threads for Ollama conversation history)
REPLY_CHAIN_ENABLED=true
REPLY_CHAIN_MAX_DEPTH=30
REPLY_CHAIN_MAX_TOKENS=16000

# Bot Interactions — when true, respond to other bots and include their messages in context
# ALLOW_BOT_INTERACTIONS=false

# Context Evaluation — global toggle and settings for Ollama-based context filtering
# CONTEXT_EVAL_ENABLED=true          # Enable/disable context evaluation globally
# CONTEXT_EVAL_MODEL=                # Ollama model for context eval (falls back to OLLAMA_MODEL)
# CONTEXT_EVAL_PROMPT=               # Custom system prompt (leave empty for built-in)
# CONTEXT_EVAL_CONTEXT_SIZE=2048     # num_ctx for context eval calls (256–131072)

# Per-pass Ollama settings — separate model, prompt, and context size for each pipeline stage
# OLLAMA_TOOL_MODEL=                 # Ollama model for tool evaluation (falls back to OLLAMA_MODEL)
# OLLAMA_TOOL_PROMPT=                # Custom system prompt for tool evaluation
# OLLAMA_TOOL_CONTEXT_SIZE=4096      # num_ctx for tool evaluation calls (256–131072)

# Final-pass settings (model and prompt are already configurable via configurator)
# OLLAMA_FINAL_PASS_MODEL=           # Ollama model for final response pass (falls back to OLLAMA_MODEL)
# OLLAMA_FINAL_PASS_PROMPT=          # Instruction appended to final pass system prompt
# OLLAMA_FINAL_PASS_CONTEXT_SIZE=4096  # num_ctx for final pass calls (256–131072)

# Logging
# DEBUG_LOGGING=false          # When true, logs full messages, prompts, and API payloads
# ABILITY_LOGGING_DETAILED=false  # Verbose abilities context logging (also enabled by DEBUG_LOGGING)
# NFL_LOGGING_LEVEL=1          # 0=summary, 1=preview (default), 2=full (also overridden by DEBUG_LOGGING)

# Image Response Settings
# When false (default), image responses contain only the attached image — no embed/internal link
IMAGE_RESPONSE_INCLUDE_EMBED=false

# Default ComfyUI Workflow (basic text-to-image; used when no custom workflow is uploaded)
# Set COMFYUI_DEFAULT_MODEL to enable. Discover options via the configurator.
# COMFYUI_DEFAULT_MODEL=
# COMFYUI_DEFAULT_WIDTH=512
# COMFYUI_DEFAULT_HEIGHT=512
# COMFYUI_DEFAULT_STEPS=20
# COMFYUI_DEFAULT_CFG=7.0
# COMFYUI_DEFAULT_SAMPLER=euler
# COMFYUI_DEFAULT_SCHEDULER=normal
# COMFYUI_DEFAULT_DENOISE=1.0
# COMFYUI_DEFAULT_SEED=-1          # -1 = random, or 0–2147483647 for a fixed seed
